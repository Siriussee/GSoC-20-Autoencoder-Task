{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "ATLAS_memorytrace.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "agJqaXXN3d9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from fastai import basic_train, basic_data\n",
        "from fastai.callbacks import ActivationStats\n",
        "import fastai\n",
        "\n",
        "import tracemalloc\n",
        "\n",
        "import pickle as pkl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRn59gcs3yHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AE_3D_200(nn.Module):\n",
        "    \"\"\"\n",
        "    An Autoencoder that compresses 4d vectors into 3d vectors. The encode-decode process is in-200-100-50-3-50-100-200-out.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_features=4):\n",
        "        \"\"\"\n",
        "        Initialize the autoencoder network, and define the layers.\n",
        "        input:\n",
        "        - n_features:int=4  The dimension of input feature, by default is 4.\n",
        "        \"\"\"\n",
        "        super(AE_3D_200, self).__init__()\n",
        "        self.en1 = nn.Linear(n_features, 200) # the 1st encoding layer, a fully-connected layer with {n_features}-d input (raw data) and 200d output \n",
        "        self.en2 = nn.Linear(200, 100) # the 2nd encoding layer, a fully-connected layer with 200d input and 100d output\n",
        "        self.en3 = nn.Linear(100, 50) # the 3rd encoding layer, a fully-connected layer with 100d input and 50d output\n",
        "        self.en4 = nn.Linear(50, 3) # the last encoding layer, a fully-connected layer with 50d input and 3d output as encoded data\n",
        "\n",
        "        self.de1 = nn.Linear(3, 50) # the 1st decoding layer, a fully-connected layer with 3d input (encoded data) and 50d output \n",
        "        self.de2 = nn.Linear(50, 100) # the 2nd decoding layer, a fully-connected layer with 50d input and 100d output \n",
        "        self.de3 = nn.Linear(100, 200) # the 3rd decoding layer, a fully-connected layer with 100d input and 200d output \n",
        "        self.de4 = nn.Linear(200, n_features) # the last decoding layer, a fully-connected layer with 200d input and {n_features}-d output (decoded data)\n",
        "        self.tanh = nn.Tanh() # the activation funcition, with a range in (-1, 1)\n",
        "\n",
        "    def encode(self, x):\n",
        "        \"\"\"\n",
        "        Encode raw data into encoded ones.\n",
        "        input:\n",
        "        - x:torch.tensor(dtype=torch.float32) The raw data that need to be encode\n",
        "        output:\n",
        "        - out:torch.tensor(dtype=torch.float32) The encoded data\n",
        "        \"\"\"\n",
        "        x = self.tanh(self.en1(x)) # encode: 4d => 200d \n",
        "        x = self.tanh(self.en2(x)) # encode: 200d => 100d\n",
        "        x = self.tanh(self.en3(x)) # encode: 100d => 50d\n",
        "        out = self.en4(x) # encode: 50d => 3d\n",
        "        return out\n",
        "\n",
        "    def decode(self, x):\n",
        "        \"\"\"\n",
        "        Decode data into decoded ones.\n",
        "        input:\n",
        "        - x:torch.tensor(dtype=torch.float32) The raw data that need to be decode\n",
        "        output:\n",
        "        - out:torch.tensor(dtype=torch.float32) The decoded data\n",
        "        \"\"\"\n",
        "        x = self.de1(self.tanh(x)) # deocde: 3d => 50d\n",
        "        x = self.de2(self.tanh(x)) # deocde: 50d => 100d\n",
        "        x = self.de3(self.tanh(x)) # deocde: 100d => 200d\n",
        "        out = self.de4(self.tanh(x)) # deocde: 3d => 50d\n",
        "        return out\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Feed forward process of training.\n",
        "        input:\n",
        "        - x:torch.tensor(dtype=torch.float32) The raw data\n",
        "        output:\n",
        "        - out:torch.tensor(dtype=torch.float32) The decoded data\n",
        "        \"\"\"\n",
        "        z = self.encode(x)\n",
        "        return self.decode(z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dtjobd-M3low",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(train_ds, valid_ds, bs):\n",
        "    \"\"\"\n",
        "    Dataloader wrapper of dataset.\n",
        "    input:\n",
        "    - train_ds:torch.tensor(dtype=torch.float32) The train dataset\n",
        "    - valid_ds:torch.tensor(dtype=torch.float32) The test dataset\n",
        "    - bs:int The size of batch\n",
        "    output:\n",
        "    - out:turple A turple of train shuffled dataloader with a batchsize in bs, and test dataloader with a batchsize in 2*bs\n",
        "    \"\"\"\n",
        "    return (\n",
        "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
        "        DataLoader(valid_ds, batch_size=bs * 2),\n",
        "    )\n",
        "\n",
        "model = AE_3D_200() # Instantiate the model implemented above\n",
        "\n",
        "loss_func = nn.MSELoss() # Use mean squared error (squared L2 norm) as loss function\n",
        "\n",
        "bn_wd = False  # Don't use weight decay for batchnorm layers\n",
        "true_wd = True  # weight decay will be used for all optimizers\n",
        "wd = 1e-6 # set the value of weight decay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qJM4uTA3d93",
        "colab_type": "code",
        "outputId": "0c944daf-bec9-4016-d60a-d8a34a415574",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "tracemalloc.start() # start tracing the memory usage in loading data\n",
        "\n",
        "# load train and test dataset from pkl files\n",
        "with open('/content/drive/My Drive/all_jets_test_4D_100_percent.pkl', 'rb') as file:\n",
        "    test = pd.DataFrame(pkl.load(file))\n",
        "\n",
        "with open('/content/drive/My Drive/all_jets_train_4D_100_percent.pkl', 'rb') as file:\n",
        "    train = pd.DataFrame(pkl.load(file))\n",
        "\n",
        "n_features = len(train.loc[0]) # get feature number (here is 4)\n",
        "\n",
        "# normalize the train and test dataset to standard one (std=1)\n",
        "train_mean = train.mean()\n",
        "train_std = train.std()\n",
        "train = (train - train_mean) / train_std\n",
        "test = (test - train_mean) / train_std\n",
        "\n",
        "# taking snapshot of current memory usage and stop tracking\n",
        "snapshot = tracemalloc.take_snapshot()\n",
        "top_stats = snapshot.statistics('lineno')\n",
        "tracemalloc.stop()\n",
        "\n",
        "# print the biggest five memory blocks\n",
        "stat = top_stats[0:5]\n",
        "for s in stat:\n",
        "    print(str(s.traceback) + \" Size: \"+ str(s.size) + \" bytes\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py:1874 Size: 4471296 bytes\n",
            "/usr/lib/python3.6/linecache.py:137 Size: 1152798 bytes\n",
            "<ipython-input-18-0fa5be27fe79>:7 Size: 895133 bytes\n",
            "<ipython-input-18-0fa5be27fe79>:4 Size: 224236 bytes\n",
            "/usr/lib/python3.6/posixpath.py:372 Size: 221563 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiekN-E23d99",
        "colab_type": "code",
        "outputId": "ccf3f166-8aa9-49bf-fe62-08315d7d869e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        }
      },
      "source": [
        "# The input data and the ground true of encode-deocded data is the same (raw data),\n",
        "# so the x (input data) and y (ground) are the copy of raw data.\n",
        "train_x = train\n",
        "test_x = test\n",
        "train_y = train_x\n",
        "test_y = test_x\n",
        "\n",
        "# Construct dataset, the data type should be declare explicitly as torch.float (torch.float32).\n",
        "# The original code does not declare, so the data\n",
        "# will be converted to torch.double (torch.float64) and lead to datatype error when encoding.\n",
        "#train_ds = TensorDataset(torch.tensor(train_x.values), torch.tensor(train_y.values)) \n",
        "#valid_ds = TensorDataset(torch.tensor(test_x.values), torch.tensor(test_y.values))\n",
        "train_ds = TensorDataset(torch.tensor(train_x.values, dtype=torch.float), torch.tensor(train_y.values, dtype=torch.float))\n",
        "valid_ds = TensorDataset(torch.tensor(test_x.values, dtype=torch.float), torch.tensor(test_y.values, dtype=torch.float))\n",
        "\n",
        "# Get dataloader\n",
        "train_dl, valid_dl = get_data(train_ds, valid_ds, bs=256)\n",
        "\n",
        "# Bind train_dl and test_dl in a data object.\n",
        "db = basic_data.DataBunch(train_dl, valid_dl)\n",
        "\n",
        "# Define the learner in Fast.ai, with a record of the mean and std of activation func (enabled by ActivationStats)\n",
        "learn = basic_train.Learner(data=db, model=model, loss_func=loss_func, wd=wd, callback_fns=ActivationStats, bn_wd=bn_wd, true_wd=true_wd)\n",
        "\n",
        "# Choose and load the trained network\n",
        "learn.load('/content/drive/My Drive/AE_3D_200_no1cycle_trainforever')\n",
        "\n",
        "# Load the model to CPU\n",
        "model.to('cpu')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AE_3D_200(\n",
              "  (en1): Linear(in_features=4, out_features=200, bias=True)\n",
              "  (en2): Linear(in_features=200, out_features=100, bias=True)\n",
              "  (en3): Linear(in_features=100, out_features=50, bias=True)\n",
              "  (en4): Linear(in_features=50, out_features=3, bias=True)\n",
              "  (de1): Linear(in_features=3, out_features=50, bias=True)\n",
              "  (de2): Linear(in_features=50, out_features=100, bias=True)\n",
              "  (de3): Linear(in_features=100, out_features=200, bias=True)\n",
              "  (de4): Linear(in_features=200, out_features=4, bias=True)\n",
              "  (tanh): Tanh()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFxtwARs3d-A",
        "colab_type": "code",
        "outputId": "c8803b7d-06a9-4f9e-ccbc-3ecb1fe90817",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# print the number of test data\n",
        "number_of_events = torch.tensor(test.values).size()[0]\n",
        "print(\"Number of events: \" + str(number_of_events))\n",
        "\n",
        "tracemalloc.start() # start tracking the memory usage of encoding\n",
        "\n",
        "# encode test data\n",
        "compressed = learn.model.encode(torch.tensor(test.values, dtype=torch.float)).detach().numpy()\n",
        "\n",
        "# taking snapshot of current memory usage and stop tracking\n",
        "snapshot = tracemalloc.take_snapshot()\n",
        "top_stats = snapshot.statistics('traceback')\n",
        "\n",
        "# pick the biggest memory block, as can be seen below, encoded data use less memory\n",
        "stat = top_stats[0]\n",
        "print(\"%s memory blocks: %.1f KiB\" % (stat.count, stat.size / 1024))\n",
        "for line in stat.traceback.format():\n",
        "    print(line)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of events: 27945\n",
            "271 memory blocks: 14.1 KiB\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/compilerop.py\", line 100\n",
            "    return compile(source, filename, symbol, self.flags | PyCF_ONLY_AST, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}